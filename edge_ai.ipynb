{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2zRfR6iUsLTJ"
   },
   "source": [
    "# Load libraries & parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UqKal6TDrK3P",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import itertools as it\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM\n",
    "from keras.utils import to_categorical\n",
    "from keras.utils import np_utils\n",
    "import seaborn as sns\n",
    "import scipy.cluster.hierarchy as spc\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from jmetal.algorithm.singleobjective.genetic_algorithm import GeneticAlgorithm\n",
    "from jmetal.operator import BinaryTournamentSelection\n",
    "from jmetal.operator.crossover import PMXCrossover\n",
    "from jmetal.operator.mutation import PermutationSwapMutation\n",
    "from jmetal.util.termination_criterion import StoppingByEvaluations\n",
    "from jmetal.util.observer import PrintObjectivesObserver\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_classif\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "from sklearn.metrics import precision_recall_fscore_support, recall_score\n",
    "from pyod.utils.data import evaluate_print\n",
    "from pyod.utils.example import visualize\n",
    "import datetime\n",
    "import pickle\n",
    "from pyod.models.auto_encoder import AutoEncoder\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import dill\n",
    "\n",
    "# parameter\n",
    "null_percentage = 0.1\n",
    "test_ratio = 0.2\n",
    "random_state = 1\n",
    "\n",
    "epochs = 200\n",
    "neurons = [32, 18, 18, 32]\n",
    "k_best = 120\n",
    "m_subsets = 3\n",
    "assert k_best % m_subsets == 0\n",
    "n_features = int(k_best/m_subsets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature correlation selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "whl5nqDp47GS",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "from jmetal.core.problem import PermutationProblem\n",
    "from jmetal.core.solution import PermutationSolution\n",
    "import random\n",
    "\n",
    "# Feature correlation selection\n",
    "class FCS(PermutationProblem):\n",
    "\n",
    "    def __init__(self, correlation, variables, subsets):\n",
    "        super(FCS, self).__init__()\n",
    "\n",
    "        self.correlation = correlation\n",
    "        self.subsets = subsets\n",
    "        self.n_features = int(variables/subsets)\n",
    "        \n",
    "        # Minimization to find most independent between variable \n",
    "        self.obj_directions = [self.MINIMIZE]\n",
    "        self.number_of_variables = variables\n",
    "        self.number_of_objectives = 1\n",
    "        self.number_of_constraints = 0\n",
    "\n",
    "    def evaluate(self, solution: PermutationSolution) -> PermutationSolution:\n",
    "        fitness = 0\n",
    "\n",
    "        for i in range(self.subsets):\n",
    "\n",
    "          end = self.n_features*(i+1)\n",
    "          start = end - self.n_features\n",
    "\n",
    "          comb = list(it.combinations(solution.variables[start:end-1], 2))\n",
    "\n",
    "          for j in comb:\n",
    "              fitness += self.correlation[j[0]][j[1]]\n",
    "\n",
    "          solution.objectives[0] = fitness\n",
    "\n",
    "        return solution\n",
    "\n",
    "    def create_solution(self) -> PermutationSolution:\n",
    "        new_solution = PermutationSolution(number_of_variables=self.number_of_variables,\n",
    "                                           number_of_objectives=self.number_of_objectives)\n",
    "        new_solution.variables = random.sample(range(self.number_of_variables), k=self.number_of_variables)\n",
    "        return new_solution\n",
    "    \n",
    "    def get_name(self):\n",
    "        return 'Feature correlation selection'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9uE90w3FtrGR"
   },
   "source": [
    "# Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 264
    },
    "colab_type": "code",
    "id": "xGbnhgONtrPe",
    "outputId": "12aa6aa7-6a74-47d3-8842-3ab07ed18bfa",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "#  reading the data\n",
    "data = pd.read_csv('shared/uci-secom.csv')\n",
    "data = data.drop(['Time'], axis = 1)\n",
    "data.loc[data['Fault'] == -1, 'Fault'] = 0\n",
    "\n",
    "# getting the shape of the data\n",
    "print(data.shape)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 336
    },
    "colab_type": "code",
    "id": "E_nD2yAcwS8s",
    "outputId": "163c171b-1050-41e6-db1b-693b3f35b453",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "output_labels = data['Fault'].value_counts()\n",
    "fault_fraction = output_labels[1]/float(output_labels[0])\n",
    "print('fault_fraction', fault_fraction)\n",
    "print(output_labels)\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VpD7VcjIslxm"
   },
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "b9DjtGf-srow"
   },
   "source": [
    "## Data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 246
    },
    "colab_type": "code",
    "id": "UAWRm575vBiQ",
    "outputId": "80df15b9-6150-4c2d-e518-525ce8c523ce",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def drop_constant_column(dataframe):\n",
    "    for column in dataframe.columns:\n",
    "        unique = dataframe[column].unique()\n",
    "        unique = unique[~np.isnan(unique)]\n",
    "        if len(unique) == 1:\n",
    "            dataframe.drop(column,inplace=True,axis=1)\n",
    "    return dataframe\n",
    "\n",
    "# Drop columns with constant value\n",
    "data = drop_constant_column(data)\n",
    "\n",
    "# Only keep columns with NaN values below percentage\n",
    "data = data.loc[:, data.isnull().mean() < null_percentage]\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EShdGNnAtHlp"
   },
   "source": [
    "## Data imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 264
    },
    "colab_type": "code",
    "id": "ynknZSZy_VHu",
    "outputId": "27956b0f-cf6a-438a-ce37-7bae27bbc199",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "data.replace(np.nan, 0, inplace = True)\n",
    "# imputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "# imputer.fit(data)\n",
    "# data = pd.DataFrame(imputer.transform(data), columns=data.columns)\n",
    "print(\"Is there any null?\", data.isnull().any().any())\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "#returns a numpy array\n",
    "x = data.values\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "x_scaled = min_max_scaler.fit_transform(x)\n",
    "data = pd.DataFrame(x_scaled, columns=data.columns, index=data.index)\n",
    "\n",
    "data.to_pickle('shared/data_preprocessed.pkl')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7vfdjs2Ltn-Z"
   },
   "source": [
    "## Dataset splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "E0RDmsqJ-oYf",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# split data by class\n",
    "fail_test_set = data[data['Fault'] == 1]\n",
    "pass_set = data[data['Fault'] == 0]\n",
    "\n",
    "n_pass_test_set = 210\n",
    "\n",
    "# randomly select and add pass data to become test set\n",
    "train_set, pass_test_set = train_test_split(pass_set, test_size = n_pass_test_set, random_state = random_state)\n",
    "test_set = pd.concat([fail_test_set, pass_test_set])\n",
    "\n",
    "# shuffle test set\n",
    "test_set = test_set.reindex(np.random.permutation(test_set.index))\n",
    "\n",
    "x_train = train_set.iloc[:, :-1]\n",
    "y_train = train_set.iloc[:, -1]\n",
    "\n",
    "x_test = test_set.iloc[:, :-1]\n",
    "y_test = test_set.iloc[:, -1]\n",
    "\n",
    "print('Train set', x_train.shape)\n",
    "print(y_train.value_counts())\n",
    "print('Test set', x_test.shape)\n",
    "print(y_test.value_counts())\n",
    "print(\"Is there any null?\", data.isnull().any().any())\n",
    "\n",
    "x_train.to_pickle('shared/x_train.pkl')\n",
    "y_train.to_pickle('shared/y_train.pkl')\n",
    "x_test.to_pickle('shared/x_test.pkl')\n",
    "y_test.to_pickle('shared/y_test.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nM84RhLJujDh"
   },
   "source": [
    "# Edge deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sX7BhzEKuo3u"
   },
   "source": [
    "## Select k features (most correlated to output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 266
    },
    "colab_type": "code",
    "id": "SNs7erLk_AZW",
    "outputId": "726cab5e-a693-48e2-9698-be615b081f72",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# using all data\n",
    "x_all = data.iloc[:, :-1]\n",
    "y_all = data.iloc[:, -1]\n",
    "\n",
    "selector = SelectKBest(f_classif, k=k_best)\n",
    "k_features = selector.fit_transform(x_all, y_all)\n",
    "k_best_index = x_all.columns[selector.get_support()]\n",
    "\n",
    "with open('shared/k_best_index.pkl', 'wb') as filename:\n",
    "    pickle.dump(k_best_index.tolist, filename)\n",
    "\n",
    "k_best_names = x_all.columns.values[selector.get_support()]\n",
    "k_best_scores = selector.scores_[selector.get_support()]\n",
    "k_best_names_scores = list(zip(k_best_names, k_best_scores))\n",
    "ns_df = pd.DataFrame(data = k_best_names_scores, columns=['Feature', 'F_Scores'])\n",
    "ns_df_sorted = ns_df.sort_values(['F_Scores', 'Feature'], ascending = [False, True])\n",
    "ns_df_sorted.plot(kind='barh', figsize=(20, 20))\n",
    "plt.savefig('shared/F_Scores.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 217
    },
    "colab_type": "code",
    "id": "5wr83dndvPGq",
    "outputId": "74f2b5a8-e93c-4cbf-a1ec-ca721f7519bb",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "data_importance = data.loc[:, k_best_index]\n",
    "data_importance.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kwd2NilMwS6v"
   },
   "source": [
    "## Generate m subsets of n features (least correlated to each other)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 872
    },
    "colab_type": "code",
    "id": "GsGeZ_YeBgHu",
    "outputId": "037e8690-a4de-4c6b-83ed-a703e166fbcb",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "#get correlations of each features in dataset\n",
    "corrmat = abs(data_importance.corr())\n",
    "top_corr_features = corrmat.index\n",
    "plt.figure(figsize=(20,20))\n",
    "#plot heat map\n",
    "g=sns.heatmap(abs(data_importance[top_corr_features].corr()),annot=False,cmap=\"RdYlGn\")\n",
    "plt.savefig('shared/P_Scores.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "2ZnhpJ9b2cTl",
    "outputId": "d68f1a15-cd14-45e8-a215-bbe8b87c6155",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "problem = FCS(abs(corrmat.values), k_best, m_subsets)\n",
    "\n",
    "algorithm = GeneticAlgorithm(\n",
    "        problem=problem,\n",
    "        population_size=100,\n",
    "        offspring_population_size=100,\n",
    "        mutation=PermutationSwapMutation(1.0 / k_best),\n",
    "        crossover=PMXCrossover(0.8),\n",
    "        selection=BinaryTournamentSelection(),\n",
    "        termination_criterion=StoppingByEvaluations(max=200000)\n",
    "    )\n",
    "\n",
    "algorithm.observable.register(observer=PrintObjectivesObserver(10000))\n",
    "\n",
    "algorithm.run()\n",
    "result = algorithm.get_result()\n",
    "\n",
    "print('Algorithm: {}'.format(algorithm.get_name()))\n",
    "print('Problem: {}'.format(problem.get_name()))\n",
    "print('Solution: {}'.format(result.variables))\n",
    "print('Fitness: {}'.format(result.objectives[0]))\n",
    "print('Computing time: {}'.format(algorithm.total_computing_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uE-mBAvqAdim"
   },
   "outputs": [],
   "source": [
    "features_allsubset = data.columns[result.variables]\n",
    "feature_subsets = []\n",
    "subset = []\n",
    "for i in features_allsubset:\n",
    "    subset.append(str(i))\n",
    "    if len(subset) == n_features:\n",
    "        feature_subsets.append(subset)\n",
    "        subset = []\n",
    "        \n",
    "print(feature_subsets)\n",
    "with open('shared/feature_subsets.pkl', 'wb') as filename:\n",
    "    pickle.dump(feature_subsets, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for subset_i in range(m_subsets):\n",
    "    temp = data.loc[:, feature_subsets[subset_i]]\n",
    "    corrmat = abs(temp.corr())\n",
    "    top_corr_features = corrmat.index\n",
    "    plt.figure(figsize=(20,20))\n",
    "    #plot heat map\n",
    "    g=sns.heatmap(abs(temp[top_corr_features].corr()),annot=False,cmap=\"RdYlGn\")\n",
    "    plt.savefig('edge-m{}/P_Scores.png'.format(subset_i+1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training each subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train on each edge\n",
    "for subset_i in range(m_subsets):\n",
    "    print(feature_subsets[subset_i])\n",
    "    x_train_subset = x_train.loc[:, feature_subsets[subset_i]]\n",
    "    x_test_subset = x_test.loc[:, feature_subsets[subset_i]]\n",
    "    \n",
    "    clf_name = 'AutoEncoder'\n",
    "    clf = AutoEncoder(epochs=epochs, hidden_neurons=neurons, contamination=fault_fraction, validation_size=0, random_state=random_state)\n",
    "    clf.fit(x_train_subset)\n",
    "    title = 'edge-m{}/autoencoder.h5'.format(subset_i+1)\n",
    "    with open(title, 'wb') as f:\n",
    "        dill.dump(clf, f, dill.HIGHEST_PROTOCOL)\n",
    "\n",
    "# Train on Cloud-k\n",
    "# x_train_subset = x_train.loc[:, k_best_index]\n",
    "# x_test_subset = x_test.loc[:, k_best_index]\n",
    "# clf_name = 'AutoEncoder'\n",
    "# clf = AutoEncoder(epochs=epochs, hidden_neurons=neurons, contamination=fault_fraction, validation_size=0, random_state=random_state)\n",
    "# clf.fit(x_train_subset)\n",
    "# title = 'cloud-k/autoencoder.h5'\n",
    "# with open(title, 'wb') as f:\n",
    "#     dill.dump(clf, f, dill.HIGHEST_PROTOCOL)\n",
    "\n",
    "# Train on Cloud-all\n",
    "# x_train_subset = x_train\n",
    "# x_test_subset = x_test\n",
    "# clf_name = 'AutoEncoder'\n",
    "# clf = AutoEncoder(epochs=epochs, hidden_neurons=neurons, contamination=fault_fraction, validation_size=0, random_state=random_state)\n",
    "# clf.fit(x_train_subset)\n",
    "# title = 'cloud-all/autoencoder.h5'\n",
    "# with open(title, 'wb') as f:\n",
    "#     dill.dump(clf, f, dill.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Server\n",
    "import socket\n",
    "import json\n",
    "import numpy as np\n",
    "import dill\n",
    "\n",
    "\n",
    "title = 'edge-m1/autoencoder.h5'\n",
    "with open(title, 'rb') as pickle_file:\n",
    "    clf = dill.load(pickle_file)\n",
    "\n",
    "host = '192.168.10.3'  # Server ip\n",
    "port = 4000\n",
    "s = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)\n",
    "s.bind((host, port))\n",
    "\n",
    "print(\"Server Started\")\n",
    "while True:\n",
    "    data, addr = s.recvfrom(1024)\n",
    "    data = json.loads(data.decode())\n",
    "    x_test_subset = np.array(data.get(\"msg\"))\n",
    "    print(\"Message from: \" + str(addr))\n",
    "    print(\"From connected user: \", len(x_test_subset))\n",
    "    x_test_subset = np.array(x_test_subset).reshape(1, -1)\n",
    "    y_test_pred = clf.predict(x_test_subset)\n",
    "    message = json.dumps({\"msg\": np.array(y_test_pred).tolist()})\n",
    "    s.sendto(message.encode(), addr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Client\n",
    "# import datetime\n",
    "# import numpy as np\n",
    "\n",
    "# m_subsets = 3\n",
    "# x_test = np.load('x_test.npy')\n",
    "# feature_subsets = np.load('feature_subsets.npy')\n",
    "# k_best_index = np.load('k_best_index.npy')\n",
    "\n",
    "# # Test on each edge\n",
    "# subset_i = 0\n",
    "# x_test_subset = x_test.loc[:, feature_subsets[subset_i]]\n",
    "# start = datetime.datetime.now()\n",
    "# #     send\n",
    "# end = datetime.datetime.now()\n",
    "# interval = end - start\n",
    "    \n",
    "# # Test on Cloud-k\n",
    "# x_test_subset = x_test.loc[:, k_best_index]\n",
    "# start = datetime.datetime.now()\n",
    "# #     send\n",
    "# end = datetime.datetime.now()\n",
    "# interval = end - start\n",
    "\n",
    "# # Test on Cloud-all\n",
    "# x_test_subset = x_test\n",
    "# start = datetime.datetime.now()\n",
    "# #     send\n",
    "# end = datetime.datetime.now()\n",
    "# interval = end - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # get the prediction on the test data\n",
    "# start = datetime.datetime.now()\n",
    "# y_test_pred = clf.predict(x_test_subset)  # outlier labels (0 or 1)\n",
    "# end = datetime.datetime.now()\n",
    "# interval_test = end - start\n",
    "\n",
    "# # Confusion matrix and classification report\n",
    "\n",
    "# matrix = confusion_matrix(y_test, y_test_pred)\n",
    "# # matrix = matrix.astype('float') / matrix.sum(axis=1)[:, np.newaxis]\n",
    "# sns.heatmap(matrix,annot=True,cbar=False, fmt='g')\n",
    "# plt.ylabel('True Label')\n",
    "# plt.xlabel('Predicted Label')\n",
    "# plt.title('Confusion matrix of subset {}'.format(subset_i+1))\n",
    "# plt.savefig('subset{}-confusionmatrix.png'.format(subset_i))\n",
    "\n",
    "# y_test_scores = clf.decision_function(x_test_subset)  # outlier scores\n",
    "# print('Subset {}'.format(subset_i+1))\n",
    "# print(y_test.value_counts())\n",
    "# print('roc_auc_score', roc_auc_score(y_test, y_test_scores))\n",
    "# print('interval test', interval_test)\n",
    "# # evaluate and print the results\n",
    "# print(\"\\nOn Test Data:\")\n",
    "# evaluate_print(clf_name, y_test, y_test_scores)\n",
    "# print(classification_report(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROC graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subset0_y_test_pred = np.load('edge-m1/y_test_pred.npy')\n",
    "# subset1_y_test_pred = np.load('edge-m2/y_test_pred.npy')\n",
    "# subset2_y_test_pred = np.load('edge-m3/y_test_pred.npy')\n",
    "# subset80_y_test_pred = np.load('subset80/y_test_pred.npy')\n",
    "# subset90_y_test_pred = np.load('subset90/y_test_pred.npy')\n",
    "\n",
    "# subset0_y_test_scores = np.load('edge-m1/y_test_scores.npy')\n",
    "# subset1_y_test_scores = np.load('edge-m2/y_test_scores.npy')\n",
    "# subset2_y_test_scores = np.load('edge-m3/y_test_scores.npy')\n",
    "# subset80_y_test_scores = np.load('cloud-k/y_test_scores.npy')\n",
    "# subset90_y_test_scores = np.load('cloud-all/y_test_scores.npy')\n",
    "\n",
    "# fpr0, tpr0, thresholds0 = roc_curve(y_test, subset0_y_test_scores)\n",
    "# fpr1, tpr1, thresholds1 = roc_curve(y_test, subset1_y_test_scores)\n",
    "# fpr2, tpr2, thresholds2 = roc_curve(y_test, subset2_y_test_scores)\n",
    "# fpr80, tpr80, thresholds80 = roc_curve(y_test, subset80_y_test_scores)\n",
    "# fpr90, tpr90, thresholds90 = roc_curve(y_test, subset90_y_test_scores)\n",
    "\n",
    "# plt.plot(fpr0, tpr0, label='ROC curve Edge-m1')\n",
    "# plt.plot(fpr1, tpr1, label='ROC curve Edge-m2')\n",
    "# plt.plot(fpr2, tpr2, label='ROC curve Edge-m3')\n",
    "# plt.plot(fpr80, tpr80, label='ROC curve Cloud-k')\n",
    "# plt.plot(fpr90, tpr90, label='ROC curve Cloud-all')\n",
    "# plt.plot([0, 1], [0, 1], 'k--', label='Baseline')\n",
    "# plt.xlabel('False Positive Rate')\n",
    "# plt.ylabel('True Positive Rate')\n",
    "# plt.title('ROC Curve')\n",
    "# plt.xlim([-0.02, 1])\n",
    "# plt.ylim([0, 1.02])\n",
    "# plt.legend(loc=\"lower right\")\n",
    "# plt.savefig('shared/roccurves.png')\n",
    "\n",
    "# plt.plot(fpr80, tpr80, label='ROC curve Cloud-k')\n",
    "# plt.plot(fpr90, tpr90, label='ROC curve Cloud-all')\n",
    "# plt.plot([0, 1], [0, 1], 'k--', label='Baseline')\n",
    "# plt.xlabel('False Positive Rate')\n",
    "# plt.ylabel('True Positive Rate')\n",
    "# plt.title('ROC Curve')\n",
    "# plt.xlim([-0.02, 1])\n",
    "# plt.ylim([0, 1.02])\n",
    "# plt.legend(loc=\"lower right\")\n",
    "# plt.savefig('shared/roccurves-cloud.png')\n",
    "\n",
    "# plt.plot(fpr0, tpr0, label='ROC curve Edge-m1')\n",
    "# plt.plot(fpr1, tpr1, label='ROC curve Edge-m2')\n",
    "# plt.plot(fpr2, tpr2, label='ROC curve Edge-m3')\n",
    "# plt.plot([0, 1], [0, 1], 'k--', label='Baseline')\n",
    "# plt.xlabel('False Positive Rate')\n",
    "# plt.ylabel('True Positive Rate')\n",
    "# plt.title('ROC Curve')\n",
    "# plt.xlim([-0.02, 1])\n",
    "# plt.ylim([0, 1.02])\n",
    "# plt.legend(loc=\"lower right\")\n",
    "# plt.savefig('shared/roccurves-edge.png')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "edge-ai.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
